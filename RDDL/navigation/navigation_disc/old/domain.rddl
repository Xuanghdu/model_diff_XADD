////////////////////////////////////////////////////////////////////
//Simple 2D navigaiton with discrete actions
//
////////////////////////////////////////////////////////////////////
domain navigation_discrete {

    requirements = {
        reward-deterministic
    };

    types {
		agent : object;
	}; 

    pvariables {


        // minerals constants
        GOAL-POS-X-MAX(agent): { non-fluent, real, default = 10 };            // goal x location
        GOAL-POS-Y-MAX(agent): { non-fluent, real, default = 10 };            // goal y location
        GOAL-POS-X-MIN(agent): { non-fluent, real, default = 8 };            // goal x location
        GOAL-POS-Y-MIN(agent): { non-fluent, real, default = 8 };            // goal y location
        MAX-POS-X(agent): { non-fluent, real, default = 10 };            // goal x location
        MAX-POS-Y(agent): { non-fluent, real, default = 10 };            // goal y location
        MOVE-DISTANCE(agent) : { non-fluent, real, default = 1 };
        GOAL-REWARD(agent) : { non-fluent, real, default = 1 };              

        // states
        pos-x(agent)    : { state-fluent, real, default = 0 };          // rover x position
        pos-y(agent)    : { state-fluent, real, default = 0 };          // rover y position

        // actions
        move-pos-x(agent)     : { action-fluent, bool, default = false };     // force input in y direction
        move-pos-y(agent)      : { action-fluent, bool, default = false };     // force input in x direction

       
    };

    cpfs {

        pos-x'(?a) =  min[pos-x(?a) + move-pos-x(?a) *  MOVE-DISTANCE(?a), MAX-POS-X(?a)];
        pos-y'(?a) =  min[pos-y(?a) + move-pos-y(?a) *  MOVE-DISTANCE(?a), MAX-POS-Y(?a)];

    };

    // negative distance to the goal
    reward = sum_{?a : agent}[
                                if ( ( (pos-x(?a) >= GOAL-POS-X-MIN(?a)) ^ (pos-x(?a) <= GOAL-POS-X-MAX(?a)) )
                                     ^ ( (pos-y(?a) >= GOAL-POS-Y-MIN(?a)) ^ (pos-y(?a) <= GOAL-POS-Y-MAX(?a)) ) )
                                    then  GOAL-REWARD(?a)
                                else pos(?a) * 0.1 *  
                              ];

    state-invariants {
    };

    action-preconditions {
    };

}
